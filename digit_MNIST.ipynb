{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "digit_MNIST.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWRIUPCfjggi"
      },
      "source": [
        "**MNIST Handwritten Digit Classification**\r\n",
        "---\r\n",
        "\r\n",
        "The MNIST dataset is an acronym that stands for the Modified National Institute of Standards and Technology dataset. Dataset contains 60,000 images of 28×28 pixel grayscale of handwritten single digits between 0 and 9. We are going to classify a given image of a handwritten digit into one of 10 classes representing integer values from 0 to 9. We are going to use Keras API for training this model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnlUO3L42XYY"
      },
      "source": [
        "### importing modules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhgG4A-MlFa3"
      },
      "source": [
        "**numpy** - NumPy is a Python library used for working with arrays.\r\n",
        "It also has functions for working in domain of linear algebra, fourier transform, and matrices.\r\n",
        "\r\n",
        "**csv** - CSV (Comma Separated Values) is a simple file format used to store tabular data, such as a spreadsheet or database. A CSV file stores tabular data (numbers and text) in plain text. Each line of the file is a data record. Each record consists of one or more fields, separated by commas. The use of the comma as a field separator is the source of the name for this file format.\r\n",
        "\r\n",
        "**tqdm** - Whether you’re installing software, loading a page or doing a transaction, it always eases your mind whenever you see that small progress bar giving you an estimation of how long the process would take to complete or render. If you have a simple progress bar in your script or code, it looks very pleasing to the eye and gives proper feedback to the user whenever he executes the code. You can use the Python external library tqdm, to create simple & hassle-free progress bars which you can add in your code and make it look lively.\r\n",
        "\r\n",
        "**os** - The OS module in python provides functions for interacting with the operating system. OS, comes under Python’s standard utility modules. This module provides a portable way of using operating system dependent functionality. The *os* and *os.path* modules include many functions to interact with the file system.\r\n",
        "\r\n",
        "**pandas** -  It provides ready to use high-performance data structures and data analysis tools. Pandas module runs on top of NumPy and it is popularly used for data science and data analytics.\r\n",
        "\r\n",
        "**keras** - Keras modules contains pre-defined classes, functions and variables which are useful for deep learning algorithm. By default, keras runs on top of TensorFlow backend."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Npj5fs-QRWga"
      },
      "source": [
        "import numpy as np\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "import keras\n",
        "from keras.preprocessing.image import load_img, img_to_array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ks45qsZ_2egV"
      },
      "source": [
        "from keras.layers import Input, Dense, Conv2D, MaxPool2D, Flatten, Dropout, BatchNormalization\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.regularizers import l2\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import plot_model\n",
        "\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import imshow\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCzQ8bU1pkHh"
      },
      "source": [
        "DATA_DIR = \"/content/drive/My Drive/DATA/identify-digits/Images/train/\"\n",
        "TRAIN_FILE = \"/content/drive/My Drive/DATA/identify-digits/train.csv\"\n",
        "TEST_FILE = \"/content/drive/My Drive/DATA/identify-digits/Test_fCbTej3_0j1gHmj.csv\"\n",
        "TEST_DIR = \"/content/drive/My Drive/DATA/identify-digits/Images/test/\"\n",
        "\n",
        "IMG_SIZE = 28\n",
        "MODEL_NAME = \"{}-{}\".format(\"1-conv-layers\", 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsW93BNy8hfz"
      },
      "source": [
        "### creating, processing, loading dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXxotQc7oWZd"
      },
      "source": [
        "**Categorical** data are variables that contain label values rather than numeric values. The number of possible values is often limited to a fixed set. Categorical variables are often called nominal.Each value represents a different category. Many machine learning algorithms cannot operate on label data directly. They require all input variables and output variables to be numeric.This means that categorical data must be converted to a numerical form. If the categorical variable is an output variable, you may also want to convert predictions by the model back into a categorical form in order to present them or use them in some application. In this case, a one-hot encoding can be applied to the integer representation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPe9jYOCvNpA"
      },
      "source": [
        "# converting categorical values in to 10 size one-hot encoded list\n",
        "def label_img(img):\n",
        "  one_hot = [0 for i in range(10)]\n",
        "  one_hot[img] = 1\n",
        "  return one_hot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLxz9BkirGTn"
      },
      "source": [
        "# function takes images from training dataset as input and converts into\n",
        "# numpy array taking 2 values(one-hot encoded labels, image in np.array)\n",
        "def create_train_data():\n",
        "  training_data = []\n",
        "  with open(TRAIN_FILE, 'r') as f:\n",
        "    csvreader = csv.reader(f)\n",
        "    fields = next(csvreader)\n",
        "\n",
        "    for row in tqdm(csvreader):\n",
        "      filename, label = row[0], label_img(int(row[1]))\n",
        "      path = DATA_DIR + filename\n",
        "      img = load_img(path, grayscale=True)\n",
        "      img = img_to_array(img)\n",
        "      training_data.append([np.array(label), img])\n",
        "    training_data = np.array(training_data)\n",
        "    f.close()\n",
        "  np.save(\"training_data.npy\", training_data)\n",
        "  return training_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_9eGHsI0t24"
      },
      "source": [
        "# function takes images from test dataset as input and converts into\n",
        "# numpy array taking 2 values(one-hot encoded labels, image in np.array)\n",
        "def create_test_data():\n",
        "  testing_data = []\n",
        "  with open(TEST_FILE, 'r') as f:\n",
        "    csvreader = csv.reader(f)\n",
        "    fields = next(csvreader)\n",
        "\n",
        "    for row in tqdm(csvreader):\n",
        "      filename = row[0]\n",
        "      path = DATA_DIR + filename\n",
        "      img = load_img(path, grayscale=True)\n",
        "      img = img_to_array(img)\n",
        "      testing_data.append(img)\n",
        "    testing_data = np.array(testing_data)\n",
        "    f.close()\n",
        "  np.save(\"testing_data.npy\", training_data)\n",
        "  return testing_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piNRncmNrUjh"
      },
      "source": [
        "we **normalize** the images is to make the model converge faster. When the data is not normalized, the shared weights of the network have different calibrations for different features, which can make the cost function to converge very slowly and ineffectively. Normalizing the data makes the cost function much easier to train."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FEMQjQ8w1LL"
      },
      "source": [
        "def load_dataset(test=False):\n",
        "  if os.path.exists('/content/drive/My Drive/DATA/identify-digits/' +  'training_data.npy'):\n",
        "    training_data = np.load('/content/drive/My Drive/DATA/identify-digits/' +  'training_data.npy', allow_pickle=True)\n",
        "  else:\n",
        "    training_data = create_train_data()\n",
        "  \n",
        "  X_train = np.array([img[1] for img in training_data])\n",
        "  X_train = X_train/255.\n",
        "  X_train = X_train.astype('float32')\n",
        "  print(\"Data Normalized!!\")\n",
        "  Y_train = np.array([img[0] for img in training_data])\n",
        "\n",
        "  print(\"Total training data images = \", X_train.shape[0])\n",
        "  print(\"Images data shape = \", X_train.shape[1:])\n",
        "  print(\"Total categories = \", Y_train.shape[1])\n",
        "\n",
        "  X_test = []\n",
        "  if(test):\n",
        "    if os.path.exists('/content/drive/My Drive/DATA/identify-digits/'+'testing_data.npy'):\n",
        "      testing_data = np.load('/content/drive/My Drive/DATA/identify-digits/'+'testing_data.npy', allow_pickle=True)\n",
        "    else:\n",
        "      testing_data = create_test_data()\n",
        "    \n",
        "    X_test = np.array([img[1] for img in testing_data])\n",
        "    X_test = X_test.reshape(X_test.shape + (1,))\n",
        "    X_test = X_test/255.\n",
        "    X_test = X_test.astype('float32') \n",
        "\n",
        "    print(\"Total testing data images = \", X_test.shape[0])\n",
        "    print(\"Images data shape = \", X_test.shape[1:])\n",
        "  \n",
        "  return X_train, Y_train, X_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKmE8Vd0zNPa"
      },
      "source": [
        "X_train, Y_train, X_test = load_dataset(False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iIUCGcn8thj"
      },
      "source": [
        "### Defining Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZcaIbsbtRQM"
      },
      "source": [
        "Since this is a small network, for the convolutional front-end, we can start with ***convolutional layer*** with a small filter size (3,3) and a number of filters (32) followed by a *max pooling layer*. The filter maps can then be flattened to provide features to the classifier.\r\n",
        "\r\n",
        "Given that the problem is a *multi-class classification* task, we know that we will require an output layer with 10 nodes in order to predict the probability distribution of an image belonging to each of the 10 classes. This will also require the use of a *softmax* activation function. Between the feature extractor and the output layer, we can add a dense layer to interpret the features, in this case with 100 nodes.\r\n",
        "\r\n",
        "All layers will use the ReLU activation function.\r\n",
        "\r\n",
        "The categorical cross-entropy loss function will be optimized, suitable for multi-class classification, and we will monitor the classification accuracy metric, which is appropriate given we have the same number of examples in each of the 10 classes.\r\n",
        "\r\n",
        "The digitModel() function below will define and return this model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xuslwb-Uzgx3"
      },
      "source": [
        "def digitModel(input_shape):\n",
        "\n",
        "  X_input = Input(input_shape)\n",
        "\n",
        "  X = Conv2D(32, kernel_size=(3,3), activation='relu', padding='same')(X_input)\n",
        "  X = Conv2D(32,  kernel_size=(3,3), activation='relu', padding='same')(X)\n",
        "  X = MaxPool2D(pool_size=(2, 2), padding='same')(X)\n",
        "\n",
        "  X = Conv2D(64, kernel_size=(3,3), activation='relu')(X)\n",
        "  X = Conv2D(64, kernel_size=(3,3), activation='relu')(X)\n",
        "  X = MaxPool2D(pool_size=(2, 2))(X)\n",
        "\n",
        "  X = Flatten()(X)\n",
        "\n",
        "  X = Dense(128, activation='relu')(X)\n",
        "\n",
        "  X = Dense(10, activation='softmax')(X)\n",
        "\n",
        "  model = Model(X_input, X)\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXgWONtK4wcF"
      },
      "source": [
        "model = digitModel(X_train.shape[1:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GcYxpZG5BCO"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PTIrXTSwEoX"
      },
      "source": [
        "---\r\n",
        "\r\n",
        "Deep learning models can take hours, days or even weeks to train. If the run is stopped unexpectedly, you can lose a lot of work. We are going to use keras.callbacks module to import ModelCheckpoint. \r\n",
        "\r\n",
        "We are going to monitor validation accuracy and we are only going to save the best."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlyEUQtE5Eh5"
      },
      "source": [
        "checkpoint = ModelCheckpoint('{}.h5'.format(MODEL_NAME),\n",
        "                            monitor='val_accuracy',\n",
        "                            save_best_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-40ya5qbxbxR"
      },
      "source": [
        "We are going to use **Adam** optimizer and **categorical_crossentropy** as our loss function.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOkfQ9Gt7X1X"
      },
      "source": [
        "#opt = Adam(learning_rate=0.0005)\n",
        "model.compile('adam',loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVLkQtPWx_bl"
      },
      "source": [
        "We have defined our model and compiled it ready for efficient computation. Now it is time to execute the model on some data. We can train or fit our model on our loaded data by calling the fit() function on the model. Training occurs over epochs and each epoch is split into batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fif85wZw7mB6"
      },
      "source": [
        "history = model.fit(X_train, Y_train, batch_size=512, epochs=7, callbacks=[checkpoint], validation_split=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVJidmUG86hP"
      },
      "source": [
        "### History"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZjpmwIWyptx"
      },
      "source": [
        "We can see, the model achieves 98% validation accuracy. These are good results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hJT3gzl8QCV"
      },
      "source": [
        "# list all data in history\n",
        "print(history.history.keys())\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RW5unWJoy7Vi"
      },
      "source": [
        "In this case, we can see that the model generally achieves a good fit, with train and test learning curves converging. There is no obvious sign of **over-fitting** or **under-fitting**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stfIQF7fzD7s"
      },
      "source": [
        "We are going to submit the file to drive  for download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eWkwzxv9XqU"
      },
      "source": [
        "TEST_FILE = \"/content/drive/My Drive/DATA/identify-digits/Test_fCbTej3_0j1gHmj.csv\"\n",
        "TEST_DIR = \"/content/drive/My Drive/DATA/identify-digits/Images/test/\"\n",
        "SUB_FILE = \"/content/drive/My Drive/DATA/identify-digits/sample_submission_npBPSZB.csv\"\n",
        "with open(TEST_FILE, \"r\") as f1:\n",
        "  with open(SUB_FILE, 'w') as f2:\n",
        "    csvreader = csv.reader(f1)\n",
        "    fields = next(csvreader)\n",
        "\n",
        "    f2.write('filename,label\\n')\n",
        "\n",
        "    for img in tqdm(csvreader):\n",
        "        file_name = img[0]\n",
        "        path = TEST_DIR + img[0]\n",
        "        img = load_img(path, grayscale=True)\n",
        "        x = img_to_array(img)\n",
        "        x = np.expand_dims(x, axis=0)\n",
        "            \n",
        "        val = np.argmax(model.predict(x))  \n",
        "        \n",
        "        f2.write('{},{}\\n'.format(file_name, int(val)) )\n",
        "    f2.close()\n",
        "  f1.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48UfNc5KzTNb"
      },
      "source": [
        "testing an image to see if it works fine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYTMfx06-5e9"
      },
      "source": [
        "TEST_DIR = \"/content/drive/My Drive/DATA/identify-digits/Images/test/\"\n",
        "path = TEST_DIR + '49000.png'\n",
        "img = load_img(path, grayscale=True)\n",
        "x = img_to_array(img)\n",
        "x = np.expand_dims(x, axis=0)\n",
        "    \n",
        "print(np.argmax(model.predict(x)))\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uz4Tf77i_ioG"
      },
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('/content/drive/My Drive/DATA/identify-digits/1-conv-layers-1.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tk512jQ4E3kt"
      },
      "source": [
        "model.evaluate(x=X_train, y=Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M56LKkwZFEAU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
